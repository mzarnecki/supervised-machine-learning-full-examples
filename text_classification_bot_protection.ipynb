{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !IMPORTANT - to run this notebook download first \"corpus\" dataset from \n",
    "# dataset https://gist.github.com/kunalj101/ad1d9c58d338e20d09ff26bcc06c4235 and put it in same folder as for this notebook\n",
    "# then import wiki-news-300d-1M.vec from https://www.kaggle.com/datasets/facebook/fasttext-wikinews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>date</th>\n",
       "      <th>ip</th>\n",
       "      <th>user_agent</th>\n",
       "      <th>is_indexing_bot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>183.136.225.56</td>\n",
       "      <td>Baiduspider+(+http://www.baidu.com/search/spid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>322967</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>176.10.98.242</td>\n",
       "      <td>CompanyHouse-PageFetcher/1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>665</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>66.249.64.4</td>\n",
       "      <td>Googlebot-Image/1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>436</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>66.249.64.5</td>\n",
       "      <td>Googlebot-Image/1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>315</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>66.249.64.6</td>\n",
       "      <td>Googlebot-Image/1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1841</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>2a01:c23:7025:ec00:3ce7:e2c4:3cb2:464f</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 10.0; Win64; x64) Appl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1842</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>2a02:908:616:1b80:1009:ef9b:1073:a888</td>\n",
       "      <td>Mozilla/5.0 (iPhone; CPU iPhone OS 15_1 like M...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1843</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>2a02:6d40:34c0:9401:54a8:698d:a68c:d9ab</td>\n",
       "      <td>Mozilla/5.0 (iPhone; CPU iPhone OS 15_1 like M...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>2a02:908:e51:d1a0:5045:44d5:a77f:bf56</td>\n",
       "      <td>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1845</th>\n",
       "      <td>6</td>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>2003:c4:1f14:7a31:8948:b237:3860:93b5</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:9...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1846 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       count        date                                       ip  \\\n",
       "0          1  2022-01-01                           183.136.225.56   \n",
       "1     322967  2022-01-01                            176.10.98.242   \n",
       "2        665  2022-01-01                              66.249.64.4   \n",
       "3        436  2022-01-01                              66.249.64.5   \n",
       "4        315  2022-01-01                              66.249.64.6   \n",
       "...      ...         ...                                      ...   \n",
       "1841       2  2022-01-02   2a01:c23:7025:ec00:3ce7:e2c4:3cb2:464f   \n",
       "1842       2  2022-01-02    2a02:908:616:1b80:1009:ef9b:1073:a888   \n",
       "1843       1  2022-01-02  2a02:6d40:34c0:9401:54a8:698d:a68c:d9ab   \n",
       "1844       2  2022-01-02    2a02:908:e51:d1a0:5045:44d5:a77f:bf56   \n",
       "1845       6  2022-01-02    2003:c4:1f14:7a31:8948:b237:3860:93b5   \n",
       "\n",
       "                                             user_agent  is_indexing_bot  \n",
       "0     Baiduspider+(+http://www.baidu.com/search/spid...                1  \n",
       "1                          CompanyHouse-PageFetcher/1.0                1  \n",
       "2                                   Googlebot-Image/1.0                1  \n",
       "3                                   Googlebot-Image/1.0                1  \n",
       "4                                   Googlebot-Image/1.0                1  \n",
       "...                                                 ...              ...  \n",
       "1841  Mozilla/5.0 (Windows NT 10.0; Win64; x64) Appl...                0  \n",
       "1842  Mozilla/5.0 (iPhone; CPU iPhone OS 15_1 like M...                0  \n",
       "1843  Mozilla/5.0 (iPhone; CPU iPhone OS 15_1 like M...                0  \n",
       "1844  Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7...                0  \n",
       "1845  Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:9...                0  \n",
       "\n",
       "[1846 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn import ensemble\n",
    "from keras import layers, models, optimizers\n",
    "import numpy\n",
    "import re\n",
    "\n",
    "trainDF = pd.read_csv('data/indexing_bots.csv', sep='\\t')\n",
    "trainDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          user_agent  word_count\n",
      "0  Baiduspider+(+http://www.baidu.com/search/spid...           1\n",
      "1                       CompanyHouse-PageFetcher/1.0           1\n",
      "2                                Googlebot-Image/1.0           1\n",
      "3                                Googlebot-Image/1.0           1\n",
      "4                                Googlebot-Image/1.0           1\n",
      "20\n",
      "1\n",
      "10.394907908992415\n"
     ]
    }
   ],
   "source": [
    "# analyze dataset\n",
    "# check number of words in each review\n",
    "train = trainDF.copy()\n",
    "train['word_count'] = train['user_agent'].apply(lambda x: len(str(x).split(\" \")))\n",
    "print(train[['user_agent','word_count']].head())\n",
    "print(train['word_count'].max())\n",
    "print(train['word_count'].min())\n",
    "print(train['word_count'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_agent</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baiduspider+(+http://www.baidu.com/search/spid...</td>\n",
       "      <td>161.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CompanyHouse-PageFetcher/1.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Googlebot-Image/1.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Googlebot-Image/1.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Googlebot-Image/1.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_agent  char_count\n",
       "0  Baiduspider+(+http://www.baidu.com/search/spid...       161.0\n",
       "1                       CompanyHouse-PageFetcher/1.0        28.0\n",
       "2                                Googlebot-Image/1.0        19.0\n",
       "3                                Googlebot-Image/1.0        19.0\n",
       "4                                Googlebot-Image/1.0        19.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check character number\n",
    "train['char_count'] = train['user_agent'].str.len() ## this also includes spaces\n",
    "train[['user_agent','char_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_agent</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baiduspider+(+http://www.baidu.com/search/spid...</td>\n",
       "      <td>161.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CompanyHouse-PageFetcher/1.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Googlebot-Image/1.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Googlebot-Image/1.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Googlebot-Image/1.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_agent  avg_word\n",
       "0  Baiduspider+(+http://www.baidu.com/search/spid...     161.0\n",
       "1                       CompanyHouse-PageFetcher/1.0      28.0\n",
       "2                                Googlebot-Image/1.0      19.0\n",
       "3                                Googlebot-Image/1.0      19.0\n",
       "4                                Googlebot-Image/1.0      19.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['user_agent'] = train['user_agent'].apply(lambda x: str(x))\n",
    "# check average word length\n",
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "train['avg_word'] = train['user_agent'].apply(lambda x: avg_word(x))\n",
    "train[['user_agent','avg_word']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          user_agent  numerics\n",
      "0  Baiduspider+(+http://www.baidu.com/search/spid...         0\n",
      "1                       CompanyHouse-PageFetcher/1.0         0\n",
      "2                                Googlebot-Image/1.0         0\n",
      "3                                Googlebot-Image/1.0         0\n",
      "4                                Googlebot-Image/1.0         0\n",
      "1\n",
      "0\n",
      "0.0021668472372697724\n"
     ]
    }
   ],
   "source": [
    "# check numeric characters number\n",
    "train['numerics'] = train['user_agent'].apply(lambda x: len([x for x in x.split(' ') if x.isnumeric()]))\n",
    "print(train[['user_agent','numerics']].head())\n",
    "print(train['numerics'].max())\n",
    "print(train['numerics'].min())\n",
    "print(train['numerics'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          user_agent  upper\n",
      "0  Baiduspider+(+http://www.baidu.com/search/spid...      0\n",
      "1                       CompanyHouse-PageFetcher/1.0      0\n",
      "2                                Googlebot-Image/1.0      0\n",
      "3                                Googlebot-Image/1.0      0\n",
      "4                                Googlebot-Image/1.0      0\n",
      "7\n",
      "0\n",
      "2.26056338028169\n"
     ]
    }
   ],
   "source": [
    "# check number of upper case letters\n",
    "train['upper'] = train['user_agent'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "print(train[['user_agent','upper']].head())\n",
    "print(train['upper'].max())\n",
    "print(train['upper'].min())\n",
    "print(train['upper'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "0\n",
      "7.399783315276273\n"
     ]
    }
   ],
   "source": [
    "# check number of punctuation characters\n",
    "train['punctuation'] = train['user_agent'].apply(lambda x: len(re.findall(r'[\\.,?!;:]', x)))\n",
    "print(train['punctuation'].max())\n",
    "print(train['punctuation'].min())\n",
    "print(train['punctuation'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    baiduspider http www baidu com search spider h...\n",
       "1                         companyhouse pagefetcher 1 0\n",
       "2                                  googlebot image 1 0\n",
       "3                                  googlebot image 1 0\n",
       "4                                  googlebot image 1 0\n",
       "Name: user_agent, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize text\n",
    "# apply lowercase\n",
    "trainDFRaw = trainDF.copy()\n",
    "trainDF['user_agent'] = trainDF['user_agent'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
    "trainDF['user_agent'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143910/1914580965.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  trainDF['user_agent'] = trainDF['user_agent'].str.replace('[^\\w\\s]',' ')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    baiduspider http www baidu com search spider h...\n",
       "1                         companyhouse pagefetcher 1 0\n",
       "2                                  googlebot image 1 0\n",
       "3                                  googlebot image 1 0\n",
       "4                                  googlebot image 1 0\n",
       "Name: user_agent, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove special characters\n",
    "trainDF['user_agent'] = trainDF['user_agent'].str.replace('[^\\w\\s]',' ')\n",
    "trainDF['user_agent'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0\n",
      " 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0\n",
      " 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0\n",
      " 0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0\n",
      " 0 0 1 0 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1\n",
      " 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0\n",
      " 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0\n",
      " 1 0 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0\n",
      " 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1\n",
      " 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1\n",
      " 0 1 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# prepare model\n",
    "# split data to train and test set\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['user_agent'], trainDF['is_indexing_bot'])\n",
    "\n",
    "# encode categorical values\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "print(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mozilla 5 0 compatible googlebot 2 1 http www google com bot html'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode text to TF-IDF numeric vectors\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['user_agent'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 561)\t0.1591449041273468\n",
      "  (0, 506)\t0.11736410662411167\n",
      "  (0, 477)\t0.15902532626640176\n",
      "  (0, 449)\t0.09066281615737472\n",
      "  (0, 437)\t0.2877033519237585\n",
      "  (0, 436)\t0.1591449041273468\n",
      "  (0, 415)\t0.1171544026990769\n",
      "  (0, 404)\t0.11729414257710087\n",
      "  (0, 386)\t0.2877033519237585\n",
      "  (0, 353)\t0.10926106377202364\n",
      "  (0, 291)\t0.18159666490288312\n",
      "  (0, 270)\t0.11729414257710087\n",
      "  (0, 217)\t0.20087939165084964\n",
      "  (0, 174)\t0.35419881740457276\n",
      "  (0, 167)\t0.09066281615737472\n",
      "  (0, 164)\t0.20108413542470366\n",
      "  (0, 131)\t0.3635129400147\n",
      "  (0, 31)\t0.3159365475507318\n",
      "  (0, 26)\t0.4195823909034112\n",
      "  (0, 0)\t0.16700018691103335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "576"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(xtrain_tfidf[100])\n",
    "len(tfidf_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# universal method for model training\n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # train model\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # generate predictions for test set\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    # evaluate model\n",
    "    scores = list(metrics.precision_recall_fscore_support(predictions, valid_y))\n",
    "    score_vals = [\n",
    "        scores[0][0],\n",
    "        scores[1][0],\n",
    "        scores[2][0]\n",
    "    ]\n",
    "    score_vals.append(metrics.accuracy_score(predictions, valid_y))\n",
    "    return score_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1384\n",
      "RF, WordLevel TF-IDF:  [0.9935275080906149, 1.0, 0.9967532467532468, 0.9956709956709957]\n",
      "0.13626670837402344\n"
     ]
    }
   ],
   "source": [
    "# MODEL - Random Forest Tree \n",
    "import time\n",
    "\n",
    "ut = time.time()\n",
    "print(len(train_x))\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "accuracy_compare = accuracy\n",
    "print (\"RF, WordLevel TF-IDF: \", accuracy)\n",
    "print(time.time() - ut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
